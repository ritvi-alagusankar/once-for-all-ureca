{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":998277,"sourceType":"datasetVersion","datasetId":547506}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# How to Get Your Specialized Neural Networks on ImageNet in Minutes With OFA Networks\n\nIn this notebook, we will demonstrate \n- how to use pretrained specialized OFA sub-networks for efficient inference on diverse hardware platforms\n- how to get new specialized neural networks on ImageNet with the OFA network within minutes.\n\n**[Once-for-All (OFA)](https://github.com/mit-han-lab/once-for-all)** is an efficient AutoML technique\nthat decouples training from search.\nDifferent sub-nets can directly grab weights from the OFA network without training.\nTherefore, getting a new specialized neural network with the OFA network is highly efficient, incurring little computation cost.\n\n![](../figures/ofa_search_cost.png)","metadata":{}},{"cell_type":"code","source":"!pat = \"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\"","metadata":{"execution":{"iopub.status.busy":"2024-03-20T13:38:43.207199Z","iopub.execute_input":"2024-03-20T13:38:43.207759Z","iopub.status.idle":"2024-03-20T13:38:44.303475Z","shell.execute_reply.started":"2024-03-20T13:38:43.207699Z","shell.execute_reply":"2024-03-20T13:38:44.302072Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/bin/bash: pat: command not found\n","output_type":"stream"}]},{"cell_type":"code","source":"!git clone https://{pat}@github.com/ritvi-alagusankar/once-for-all-ureca","metadata":{"execution":{"iopub.status.busy":"2024-03-20T13:38:44.305536Z","iopub.execute_input":"2024-03-20T13:38:44.306050Z","iopub.status.idle":"2024-03-20T13:38:45.404797Z","shell.execute_reply.started":"2024-03-20T13:38:44.306016Z","shell.execute_reply":"2024-03-20T13:38:45.403393Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"fatal: destination path 'once-for-all-ureca' already exists and is not an empty directory.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 1. Preparation\nLet's first install all the required packages:","metadata":{}},{"cell_type":"code","source":"print('Installing PyTorch...')\n! pip install torch 1>/dev/null\nprint('Installing torchvision...')\n! pip install torchvision 1>/dev/null\nprint('Installing numpy...')\n! pip install numpy 1>/dev/null\n# thop is a package for FLOPs computing.\nprint('Installing thop (FLOPs counter) ...')\n! pip install thop 1>/dev/null\n# ofa is a package containing training code, pretrained specialized models and inference code for the once-for-all networks.\nprint('Installing OFA...')\n! pip install ofa 1>/dev/null\n# tqdm is a package for displaying a progress bar.\nprint('Installing tqdm (progress bar) ...')\n! pip install tqdm 1>/dev/null\nprint('Installing matplotlib...')\n! pip install matplotlib 1>/dev/null\nprint('All required packages have been successfully installed!')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install gdown","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then, we can import the packages used in this tutorial:","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nfrom torchvision import transforms, datasets\nimport numpy as np\nimport time\nimport random\nimport math\nimport copy\nfrom matplotlib import pyplot as plt\n\nfrom ofa.model_zoo import ofa_net\nfrom ofa.utils import download_url\n\n# from ofa.tutorial.accuracy_predictor import AccuracyPredictor\n# from ofa.tutorial.flops_table import FLOPsTable\n# from ofa.tutorial.latency_table import LatencyTable\n# from ofa.tutorial.evolution_finder import EvolutionFinder\n# from ofa.tutorial.imagenet_eval_helper import evaluate_ofa_subnet, evaluate_ofa_specialized\nfrom ofa.tutorial import AccuracyPredictor, FLOPsTable, LatencyTable, EvolutionFinder\nfrom ofa.tutorial import evaluate_ofa_subnet, evaluate_ofa_specialized\n\n# set random seed\nrandom_seed = 1\nrandom.seed(random_seed)\nnp.random.seed(random_seed)\ntorch.manual_seed(random_seed)\nprint('Successfully imported all packages and configured random seed to %d!'%random_seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now it's time to determine which device to use for neural network inference in the rest of this tutorial. If your machine is equipped with GPU(s), we will use the GPU by default. Otherwise, we will use the CPU.","metadata":{}},{"cell_type":"code","source":"#os.environ['CUDA_VISIBLE_DEVICES'] = '0'\ncuda_available = torch.cuda.is_available()\nif cuda_available:\n    torch.backends.cudnn.enabled = True\n    torch.backends.cudnn.benchmark = True\n    torch.cuda.manual_seed(random_seed)\n    print('Using GPU.')\nelse:\n    print('Using CPU.')","metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Good! Now you have successfully configured the environment! It's time to import the **OFA network** for the following experiments.\nThe OFA network used in this tutorial is built upon MobileNetV3 with width multiplier 1.2, supporting elastic depth (2, 3, 4) per stage, elastic expand ratio (3, 4, 6), and elastic kernel size (3, 5 7) per block.","metadata":{}},{"cell_type":"code","source":"ofa_network = ofa_net('ofa_mbv3_d234_e346_k357_w1.2', pretrained=True)\nprint('The OFA Network is ready.')","metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let's build the ImageNet dataset and the corresponding dataloader. Notice that **if you're using the CPU,\nwe will skip ImageNet evaluation by default** since it will be very slow.\nIf you are using the GPU, in case you don't have the full dataset,\nwe will download a subset of ImageNet which contains 2,000 images (~250M) for testing.\nIf you do have the full ImageNet dataset on your machine, just specify it in `imagenet_data_path` and the downloading script will be skipped.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"if cuda_available:\n    # path to the ImageNet dataset\n    print(\"Please input the path to the ImageNet dataset.\\n\")\n    imagenet_data_path = input()\n\n    # if 'imagenet_data_path' is empty, download a subset of ImageNet containing 2000 images (~250M) for test\n    if not os.path.isdir(imagenet_data_path):\n        os.makedirs(imagenet_data_path, exist_ok=True)\n        import gdown\n        gdown.download(\"https://drive.google.com/uc?id=18gEZh9xzsiHi9yqWaCcxqKbYrjW4IPRB\", \"data/imagenet_1k.zip\")\n        ! cd data && unzip imagenet_1k 1>/dev/null && cd ..\n        ! cp -r data/imagenet_1k/* $imagenet_data_path\n        ! rm -rf data\n        print('%s is empty. Download a subset of ImageNet for test.' % imagenet_data_path)\n\n    print('The ImageNet dataset files are ready.')\nelse:\n    print('Since GPU is not found in the environment, we skip all scripts related to ImageNet evaluation.')","metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now you have configured the dataset. Let's build the dataloader for evaluation.\nAgain, this will be skipped if you are in a CPU environment.","metadata":{}},{"cell_type":"code","source":"if cuda_available:\n    # The following function build the data transforms for test\n    def build_val_transform(size):\n        return transforms.Compose([\n            transforms.Resize(int(math.ceil(size / 0.875))),\n            transforms.CenterCrop(size),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]\n            ),\n        ])\n\n    data_loader = torch.utils.data.DataLoader(\n        datasets.ImageFolder(\n            root=os.path.join(imagenet_data_path, 'val'),\n            transform=build_val_transform(224)\n        ),\n        batch_size=250,  # test batch size\n        shuffle=True,\n        num_workers=16,  # number of workers for the data loader\n        pin_memory=True,\n        drop_last=False,\n    )\n    print('The ImageNet dataloader is ready.')\nelse:\n    data_loader = None\n    print('Since GPU is not found in the environment, we skip all scripts related to ImageNet evaluation.')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Using Pretrained Specialized OFA Sub-Networks\n![](../figures/select_subnets.png)\nThe specialized OFA sub-networks are \"small\" networks sampled from the \"big\" OFA network as is indicated in the figure above.\nThe OFA network supports over $10^{19}$ sub-networks simultaneously, so that the deployment cost for multiple scenarios can be saved by 16$\\times$ to 1300$\\times$ under 40 deployment scenarios.\nNow, let's play with some of the sub-networks through the following interactive command line prompt (**Notice that for CPU users, this will be skipped**).\nWe recommend you to try a smaller sub-network (e.g., the sub-network for pixel1 with 20ms inference latency constraint) so that it takes less time to evaluate the model on ImageNet.","metadata":{}},{"cell_type":"code","source":"if cuda_available:\n    net_id = evaluate_ofa_specialized(imagenet_data_path, data_loader)\n    print('Finished evaluating the pretrained sub-network: %s!' % net_id)\nelse:\n    print('Since GPU is not found in the environment, we skip all scripts related to ImageNet evaluation.')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3 Efficient Deployment with OFA Networks\n\nYou have now successfully prepared the whole environment for the experiment!\nIn the next step, we will introduce **how to get efficient, specialized neural networks within minutes**\npowered by the OFA network.\n\n### 3.1 Latency-Constrained Efficient Deployment on Samsung Note10\n\nThe key components of very fast neural network deployment are **accuracy predictors** and **efficiency predictors**.\nFor the accuracy predictor, it predicts the Top-1 accuracy of a given sub-network on a **holdout validation set**\n(different from the official 50K validation set) so that we do **NOT** need to run very costly inference on ImageNet\nwhile searching for specialized models. Such an accuracy predictor is trained using an accuracy dataset built with the OFA network.\n\n![](../figures/predictor_based_search.png)","metadata":{}},{"cell_type":"code","source":"# accuracy predictor\naccuracy_predictor = AccuracyPredictor(\n    pretrained=True,\n    device='cuda:0' if cuda_available else 'cpu'\n)\n\nprint('The accuracy predictor is ready!')\nprint(accuracy_predictor.model)","metadata":{"pycharm":{"name":"#%%\n"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we have the powerful **accuracy predictor**. We then introduce two types of **efficiency predictors**: the latency predictor and the FLOPs predictor. \n\nThe intuition of having efficiency predictors, especially the latency predictor, is that measuring the latency of a sub-network on-the-fly is also costly, especially for mobile devices.\nThe latency predictor is designed to eliminate this cost.\nLet's load a latency predictor we built beforehand for the Samsung Note10.","metadata":{}},{"cell_type":"code","source":"target_hardware = 'note10'\nlatency_table = LatencyTable(device=target_hardware)\nprint('The Latency lookup table on %s is ready!' % target_hardware)","metadata":{"pycharm":{"name":"#%%\n"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So far, we have defined both the accuracy predictor and the latency predictor. Now, let's experience **very fast model specialization** on Samsung Note10 with these two powerful predictors! \n\n**Notice**: The predicted accuracy is on a holdout validation set of 10K images, not the official 50K validation set.\nBut they are highly positive-correlated.","metadata":{}},{"cell_type":"code","source":"\"\"\" Hyper-parameters for the evolutionary search process\n    You can modify these hyper-parameters to see how they influence the final ImageNet accuracy of the search sub-net.\n\"\"\"\nlatency_constraint = 25  # ms, suggested range [15, 33] ms\nP = 100  # The size of population in each generation\nN = 500  # How many generations of population to be searched\nr = 0.25  # The ratio of networks that are used as parents for next generation\nparams = {\n    'constraint_type': target_hardware, # Let's do FLOPs-constrained search\n    'efficiency_constraint': latency_constraint,\n    'mutate_prob': 0.1, # The probability of mutation in evolutionary search\n    'mutation_ratio': 0.5, # The ratio of networks that are generated through mutation in generation n >= 2.\n    'efficiency_predictor': latency_table, # To use a predefined efficiency predictor.\n    'accuracy_predictor': accuracy_predictor, # To use a predefined accuracy_predictor predictor.\n    'population_size': P,\n    'max_time_budget': N,\n    'parent_ratio': r,\n}\n\n# build the evolution finder\nfinder = EvolutionFinder(**params)\n\n# start searching\nresult_lis = []\nst = time.time()\nbest_valids, best_info = finder.run_evolution_search()\nresult_lis.append(best_info)\ned = time.time()\nprint('Found best architecture on %s with latency <= %.2f ms in %.2f seconds! '\n      'It achieves %.2f%s predicted accuracy with %.2f ms latency on %s.' %\n      (target_hardware, latency_constraint, ed-st, best_info[0] * 100, '%', best_info[-1], target_hardware))\n\n# visualize the architecture of the searched sub-net\n_, net_config, latency = best_info\nofa_network.set_active_subnet(ks=net_config['ks'], d=net_config['d'], e=net_config['e'])\nprint('Architecture of the searched sub-net:')\nprint(ofa_network.module_str)","metadata":{"pycharm":{"name":"#%%    \n"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great! You get your specialized neural network with **just a few seconds**!\nYou can go back to the last cell and modify the hyper-parameters to see how they affect the search time and the accuracy.\n\nWe also provided an interface below to draw a figure comparing your searched specialized network and other efficient neural networks such as MobileNetV3 and ProxylessNAS.\n\n**Notice**: For ease of comparison, we recommend you to choose a latency constraint between 15ms and 33ms.","metadata":{}},{"cell_type":"code","source":"# evaluate the searched model on ImageNet\nif cuda_available:\n    top1s = []\n    latency_list = []\n    for result in result_lis:\n        _, net_config, latency = result\n        print('Evaluating the sub-network with latency = %.1f ms on %s' % (latency, target_hardware))\n        top1 = evaluate_ofa_subnet(\n            ofa_network,\n            imagenet_data_path,\n            net_config,\n            data_loader,\n            batch_size=250,\n            device='cuda:0' if cuda_available else 'cpu')\n        top1s.append(top1)\n        latency_list.append(latency)\n\n    plt.figure(figsize=(4,4))\n    plt.plot(latency_list, top1s, 'x-', marker='*', color='darkred',  linewidth=2, markersize=8, label='OFA')\n    plt.plot([26, 45], [74.6, 76.7], '--', marker='+', linewidth=2, markersize=8, label='ProxylessNAS')\n    plt.plot([15.3, 22, 31], [73.3, 75.2, 76.6], '--', marker='>', linewidth=2, markersize=8, label='MobileNetV3')\n    plt.xlabel('%s Latency (ms)' % target_hardware, size=12)\n    plt.ylabel('ImageNet Top-1 Accuracy (%)', size=12)\n    plt.legend(['OFA', 'ProxylessNAS', 'MobileNetV3'], loc='lower right')\n    plt.grid(True)\n    plt.show()\n    print('Successfully draw the tradeoff curve!')\nelse:\n    print('Since GPU is not found in the environment, we skip all scripts related to ImageNet evaluation.')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Notice:** You can further significantly improve the accuracy of the searched sub-net by fine-tuning it on the ImageNet training set.\nOur results after fine-tuning for 25 epochs are as follows:\n![](../figures/diverse_hardware.png)\n\n\n### 3.2 FLOPs-Constrained Efficient Deployment\n\nNow, let's proceed to the final experiment of this tutorial: efficient deployment under FLOPs constraint. We use the same accuracy predictor since accuracy predictors are agnostic to the types of efficiency constraint (mobile latency / FLOPs). For the efficiency predictor, we change the latency lookup table to a flops lookup table. You can run the code below to setup it in a few seconds.","metadata":{}},{"cell_type":"code","source":"flops_lookup_table = FLOPsTable(\n    device='cuda:0' if cuda_available else 'cpu',\n    batch_size=1,\n)\nprint('The FLOPs lookup table is ready!')","metadata":{"pycharm":{"is_executing":true,"name":"#%%\n"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, you can start a FLOPs-constrained neural architecture search. Here, we directly generate **an entire tradeoff curve** for you. Please notice that the time it takes to get each data point will get longer and longer (but always less than 30 seconds) because smaller FLOPs-constraint is more difficult to meet.\n\nIf you are using CPUs, you will be able to see a \"predicted holdout validation set accuracy - FLOPs\" tradeoff curve, which can be obtained in just a  minute.\n\nIf you are using GPUs, besides the curve mentioned above, we will also evaluate all the models you designed on the ImageNet validation set (**Again, it will be better if you have the full ImageNet validation set**, but it's also OK if you downloaded the subset above) and generate an \"ImageNet 50K validation set accuracy - FLOPs\" tradeoff curve. We will also plot competing methods such as ProxylessNAS, MobileNetV3, and EfficientNet in this curve for your reference. The estimated time to get the two curves is less than 10 minutes.\n\nPlease notice that it usually takes ** hundreds/thousands of hours** to generate an accuracy-FLOPs tradeoff curve for ProxylessNAS / MobileNetV3 / EfficientNet, but generating the tradeoff curve for our OFA takes just a few minutes, as you will experience soon.","metadata":{}},{"cell_type":"code","source":"\"\"\" Hyper-parameters for the evolutionary search process\n    You can modify these hyper-parameters to see how they influence the final ImageNet accuracy of the search sub-net.\n\"\"\"\nP = 100  # The size of population in each generation\nN = 500  # How many generations of population to be searched\nr = 0.25  # The ratio of networks that are used as parents for next generation\nparams = {\n    'constraint_type': 'flops', # Let's do FLOPs-constrained search\n    'efficiency_constraint': 600,  # FLops constraint (M), suggested range [150, 600]\n    'mutate_prob': 0.1, # The probability of mutation in evolutionary search\n    'mutation_ratio': 0.5, # The ratio of networks that are generated through mutation in generation n >= 2.\n    'efficiency_predictor': flops_lookup_table, # To use a predefined efficiency predictor.\n    'accuracy_predictor': accuracy_predictor, # To use a predefined accuracy_predictor predictor.\n    'population_size': P,\n    'max_time_budget': N,\n    'parent_ratio': r,\n}\n\n# build the evolution finder\nfinder = EvolutionFinder(**params)\n\n# start searching\nresult_lis = []\nfor flops in [600, 400, 350]:\n    st = time.time()\n    finder.set_efficiency_constraint(flops)\n    best_valids, best_info = finder.run_evolution_search()\n    ed = time.time()\n    # print('Found best architecture at flops <= %.2f M in %.2f seconds! It achieves %.2f%s predicted accuracy with %.2f MFLOPs.' % (flops, ed-st, best_info[0] * 100, '%', best_info[-1]))\n    result_lis.append(best_info)\n\nplt.figure(figsize=(4,4))\nplt.plot([x[-1] for x in result_lis], [x[0] * 100 for x in result_lis], 'x-', marker='*', color='darkred',  linewidth=2, markersize=8, label='OFA')\nplt.xlabel('FLOPs (M)', size=12)\nplt.ylabel('Predicted Holdout Top-1 Accuracy (%)', size=12)\nplt.legend(['OFA'], loc='lower right')\nplt.grid(True)\nplt.show()","metadata":{"pycharm":{"name":"#%%\n"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's evaluate the searched models on ImageNet if GPU is available:","metadata":{}},{"cell_type":"code","source":"if cuda_available:\n    # test the searched model on the test dataset (ImageNet val)\n    top1s = []\n    flops_lis = []\n    for result in result_lis:\n        _, net_config, flops = result\n        print('Evaluating the sub-network with FLOPs = %.1fM' % flops)\n        top1 = evaluate_ofa_subnet(\n            ofa_network,\n            imagenet_data_path,\n            net_config,\n            data_loader,\n            batch_size=250,\n            device='cuda:0' if cuda_available else 'cpu')\n        print('-' * 45)\n        top1s.append(top1)\n        flops_lis.append(flops)\n\n    plt.figure(figsize=(8,4))\n    plt.subplot(1, 2, 1)\n    plt.plot([x[-1] for x in result_lis], [x[0] * 100 for x in result_lis], 'x-', marker='*', color='darkred',  linewidth=2, markersize=8, label='OFA')\n    plt.xlabel('FLOPs (M)', size=12)\n    plt.ylabel('Predicted Holdout Top-1 Accuracy (%)', size=12)\n    plt.legend(['OFA'], loc='lower right')\n    plt.grid(True)\n\n    plt.subplot(1, 2, 2)\n    plt.plot(flops_lis, top1s, 'x-', marker='*', color='darkred',  linewidth=2, markersize=8, label='OFA')\n    plt.plot([320, 581], [74.6, 76.7], '--', marker='+', linewidth=2, markersize=8, label='ProxylessNAS')\n    plt.plot([219, 343], [75.2, 76.6], '--', marker='^', linewidth=2, markersize=8, label='MobileNetV3')\n    plt.plot([390, 700], [76.3, 78.8], '--', marker='>', linewidth=2, markersize=8, label='EfficientNet')\n    plt.xlabel('FLOPs (M)', size=12)\n    plt.ylabel('ImageNet Top-1 Accuracy (%)', size=12)\n    plt.legend(['OFA', 'ProxylessNAS', 'MobileNetV3', 'EfficientNet'], loc='lower right')\n    plt.grid(True)\n    plt.show()","metadata":{"pycharm":{"name":"#%%\n"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Notice:** Again, you can further improve the accuracy of the search sub-net by fine-tuning it on ImageNet.\nThe final accuracy is much better than training the same architecture from scratch.\nOur results are as follows:\n![](../figures/imagenet_80_acc.png)\n![](../figures/cnn_imagenet_new.png)\n\nCongratulations! You've finished all the content of this tutorial!\nHope you enjoy playing with the OFA Networks. If you are interested,  please refer to our paper and GitHub Repo for further details.\n\n## Reference\n[1] CVPR'20 tutorial: **AutoML for TinyML with Once-for-All Network**. [[talk]](https://www.youtube.com/watch?v=fptQ_eJ3Uc0&feature=youtu.be).\n\n[1] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang and Song Han.\n**Once for All: Train One Network and Specialize It for Efficient Deployment**. In *ICLR* 2020.\n[[paper]](https://arxiv.org/abs/1908.09791), [[code]](https://github.com/mit-han-lab/once-for-all), [[talk]](https://www.youtube.com/watch?v=a_OeT8MXzWI).\n\n[2] Han Cai, Ligeng Zhu and Song Han. **ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware**.\nIn *ICLR* 2019. [[paper]](https://arxiv.org/abs/1812.00332), [[code]](https://github.com/MIT-HAN-LAB/ProxylessNAS).\n","metadata":{}}]}